{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAt-K2qgcIou"
   },
   "source": [
    "# Single Perceptron Neural Networks for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZYK-0rin5x7"
   },
   "source": [
    "Welcome to your week 3 programming assignment. Now you are ready to apply matrix multiplication by building your first neural network with a single perceptron.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Implement a neural network with a single perceptron and one input node for simple linear regression\n",
    "- Implement forward propagation using matrix multiplication\n",
    "- Implement a neural network with a single perceptron and two input nodes for multiple linear regression\n",
    "\n",
    "*Note*: Backward propagation with the parameters update requires understanding of Calculus. It is discussed in details in the Course \"Calculus\" (Course 2 in the Specialization \"Mathematics for Machine Learning\"). In this assignment backward propagation and parameters update functions are hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Simple Linear Regression](#1)\n",
    "  - [ 1.1 - Simple Linear Regression Model](#1.1)\n",
    "  - [ 1.2 - Neural Network Model with a Single Perceptron and One Input Node](#1.2)\n",
    "  - [ 1.3 - Dataset](#1.3)\n",
    "    - [ Exercise 1](#ex01)\n",
    "- [ 2 - Implementation of the Neural Network Model for Linear Regression](#2)\n",
    "  - [ 2.1 - Defining the Neural Network Structure](#2.1)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 2.2 - Initialize the Model's Parameters](#2.2)\n",
    "    - [ Exercise 3](#ex03)\n",
    "  - [ 2.3 - The Loop](#2.3)\n",
    "    - [ Exercise 4](#ex04)\n",
    "  - [ 2.4 - Integrate parts 2.1, 2.2 and 2.3 in nn_model()](#2.4)\n",
    "    - [ Exercise 5](#ex05)\n",
    "- [ 3 - Multiple Linear Regression](#3)\n",
    "  - [ 3.1 - Multipe Linear Regression Model](#3.1)\n",
    "  - [ 3.2 - Neural Network Model with a Single Perceptron and Two Input Nodes](#3.2)\n",
    "  - [ 3.3 - Dataset](#3.3)\n",
    "  - [ 3.4 - Performance of the Neural Network Model for Multiple Linear Regression](#3.4)\n",
    "    - [ Exercise 6](#ex06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI8PBrk_2Z4V"
   },
   "source": [
    "## Packages\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# A function to create a dataset.\n",
    "from sklearn.datasets import make_regression\n",
    "# A library for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "# Some functions defined specifically for this notebook.\n",
    "import w3_tools\n",
    "\n",
    "# Output of plotting commands is displayed inline within the Jupyter notebook.\n",
    "%matplotlib inline \n",
    "\n",
    "# Set a seed so that the results are consistent.\n",
    "np.random.seed(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the unit tests defined for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w3_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Simple Linear Regression\n",
    "\n",
    "**Linear regression** is a linear approach for modelling the relationship between a scalar response (**dependent variable**) and one or more explanatory variables (**independent variables**). The case of one independent variable is called **simple linear regression**; for more than one, it is called **multiple linear regression**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 - Simple Linear Regression Model\n",
    "\n",
    "Simple linear regression model can be written as\n",
    "\n",
    "$$\\hat{y} = wx + b,\\tag{1}$$\n",
    "\n",
    "where $\\hat{y}$ is a prediction of dependent variable $y$ based on independent variable $x$ using a line equation with the slope $w$ and intercept $b$. \n",
    "\n",
    "Given a set of training data points $(x_1, y_1)$, ..., $(x_m, y_m)$, the aim is to find the \"best\" fitting line - such parameters $w$ and $b$ that the differences between original values $y_i$ and predicted values $\\hat{y}_i = wx_i + b$ are minimum.\n",
    "\n",
    "You can use a simple neural network model to do that. Vector algebra will be used in the core of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Neural Network Model with a Single Perceptron and One Input Node\n",
    "\n",
    "The simplest neural network model has only one **perceptron**. It takes some inputs and calculates the output value.\n",
    "\n",
    "The perceptron components are shown in the following scheme:\n",
    "\n",
    "<img src=\"images/nn_model_linear_regression_simple.png\" style=\"width:400px;\">\n",
    "\n",
    "The smallest construction block of neural networks is called a **node**. Some nodes store numbers from the input and others store the calculated values. **Input nodes** (here there is only one input node $x$) contain the input to the network which consists of your data. These nodes are set as an **input layer** of the network.\n",
    "\n",
    "**Weight** ($w$) and **bias** ($b$) are the parameters which will get updated when you will **train** the model. They are initialized to some random value or set to 0 and updated as the training progresses. The bias is analogous to a weight independent of any input node. It makes the model more flexible.\n",
    "\n",
    "The perceptron output calculation is straightforward: first compute the product of $x$ and weight $w$ and the add the bias:\n",
    "\n",
    "$$z = w x + b\\tag{2}$$\n",
    "\n",
    "The **output layer** of the single perceptron has only one node $\\hat{y} = z$.\n",
    "\n",
    "Putting it all together, mathematically the single perceptron neural network model can be expressed as:\n",
    "\n",
    "\\begin{align}\n",
    "z^{(i)} &=  w x^{(i)} + b,\\\\\n",
    "\\hat{y}^{(i)} &= z^{(i)},\n",
    "\\tag{3}\\end{align}\n",
    "\n",
    "where $x^{(i)}$ represents the $i$-th training example and $\\hat{y}^{(i)}$ will be the prediction based on that example, $i = 1, \\dots, m$.\n",
    "\n",
    "If you have $m$ training examples, vector operations will give you a chance to perform the calculations simultaniously for all of them! Organise all training examples as a vector $X$ of size ($1 \\times m$). Then perform scalar multiplication of $X$ ($1 \\times m$) by a scalar $w$, adding $b$, which will be broadcasted to the vector of size ($1 \\times m$):\n",
    "\n",
    "\\begin{align}\n",
    "Z &=  w X + b,\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\tag{4}\\end{align}\n",
    "\n",
    "This significantly speeds up the calculations for the larger training sets! This set of calculations is called **forward propagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can compare the resulting vector of the predictions $\\hat{Y}$ ($1 \\times m$) with the original vector of data $Y$. This can be done with the so called **cost function** that measures how close your vector of predictions to the training data. It evaluates how well the parameters $w$ and $b$ work to solve the problem. There are many different cost functions available depending on the nature of your problem. For your simple neural network you can calculate it as:\n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.\\tag{5}$$\n",
    "\n",
    "The aim is to minimize the cost function during the training, which will minimize the differences between original values $y_i$ and predicted values $\\hat{y}_i$ (division by $2m$ is taken just for scaling purposes).\n",
    "\n",
    "When your weights were just initialized with some random values, and no training was done yet, you can't expect good results.\n",
    "\n",
    "The next step is to adjust the weights and bias, in order to minimize the cost function. This process is called **backward propagation** and is done iteratively: you update the parameters with a small change and repeat the process.\n",
    "\n",
    "*Note*: Backward propagation is not covered in this Course - it will be discussed in the next Course of this Specialization.\n",
    "\n",
    "The general **methodology** to build a neural network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation (calculate the perceptron output),\n",
    "    - Implement backward propagation (to get the required corrections for the parameters),\n",
    "    - Update parameters.\n",
    "4. Make predictions.\n",
    "\n",
    "You often build helper functions to compute steps 1-3 and then merge them into one function you call `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 - Dataset\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will create $m=30$ data points $(x_1, y_1)$, ..., $(x_m, y_m)$ and save them in `NumPy` arrays `X` and `Y` of a shape $(1 \\times m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset X:\n",
      "[[ 0.3190391  -1.07296862  0.86540763 -0.17242821  1.14472371  0.50249434\n",
      "  -2.3015387  -0.68372786 -0.38405435 -0.87785842 -2.06014071 -1.10061918\n",
      "  -1.09989127  1.13376944  1.74481176 -0.12289023 -0.93576943  1.62434536\n",
      "   1.46210794  0.90159072 -0.7612069   0.53035547 -0.52817175 -0.26788808\n",
      "   0.58281521  0.04221375  0.90085595 -0.24937038 -0.61175641 -0.3224172 ]]\n",
      "Training dataset Y\n",
      "[[  3.08130585 -48.2639338   25.93592732   1.87178372  45.4428258\n",
      "    6.88380644 -72.44927625 -16.20740827 -12.15967691 -25.0337406\n",
      "  -66.76530644 -35.83422144 -27.17695128  53.04737598  52.96856006\n",
      "   10.13384942 -20.19197583  36.16208808  58.99412194  38.42855536\n",
      "  -32.81240623   6.77988325 -15.82439642 -20.62478531  18.61129032\n",
      "    9.78715027  31.80735422  -4.11214063 -11.04200012 -15.08634424]]\n"
     ]
    }
   ],
   "source": [
    "m = 30\n",
    "\n",
    "X, Y = make_regression(n_samples=m, n_features=1, noise=10, random_state=1)\n",
    "\n",
    "X = X.reshape((1, m))\n",
    "Y = Y.reshape((1, m))\n",
    "\n",
    "print('Training dataset X:')\n",
    "print(X)\n",
    "print('Training dataset Y')\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$y$')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,  Y, c=\"black\")\n",
    "\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "What is the `shape` of the variables `X` and `Y`? In addition, how many training examples do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>How do you get the shape of a NumPy array?</li>\n",
    "    <li>The coordinates x1, x2 were saved in the columns of the array X</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X: (1, 30)\n",
      "The shape of Y: (1, 30)\n",
      "I have m = 30 training examples!\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (~ 3 lines of code)\n",
    "# Shape of variable X.\n",
    "shape_X = (1,30)\n",
    "# Shape of variable Y.\n",
    "shape_Y = (1,30)\n",
    "# Training set size.\n",
    "m = 30\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X: ' + str(shape_X))\n",
    "print ('The shape of Y: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```Python\n",
    "The shape of X: (1, 30)\n",
    "The shape of Y: (1, 30)\n",
    "I have m = 30 training examples!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_shapes(shape_X, shape_Y, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Implementation of the Neural Network Model for Linear Regression\n",
    "\n",
    "Let's setup the neural network in a way which will allow to extend this simple case of a model to more complicated structures later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Defining the Neural Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Define two variables:\n",
    "- `n_x`: the size of the input layer\n",
    "- `n_y`: the size of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    Use shapes of X and Y to find n_x and n_y:\n",
    "    <li>the size of the input layer n_x equals to the size of the input vectors placed in the columns of the array X,</li>\n",
    "    <li>the outpus for each of the data point will be saved in the columns of the the array Y.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    # Size of input layer.\n",
    "    n_x = X.shape[0]\n",
    "    # Size of output layer.\n",
    "    n_y = Y.shape[0]\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 1\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_y) = layer_sizes(X, Y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```Python\n",
    "The size of the input layer is: n_x = 1\n",
    "The size of the output layer is: n_y = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_layer_sizes(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Initialize the Model's Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "Implement the function `initialize_parameters()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.\n",
    "- You will initialize the weights matrix with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- You will initialize the bias vector as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W -- weight matrix of shape (n_y, n_x)\n",
    "                    b -- bias value set as a vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    W = np.random.randn(n_y,n_x)\n",
    "    b = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W.shape == (n_y, n_x))\n",
    "    assert (b.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[1.78862847]]\n",
      "b = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_y)\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the elements of the array W maybe be different due to random initialization. You can try to restart the kernel to get the same values.\n",
    "\n",
    "```Python\n",
    "W = [[0.01788628]]\n",
    "b = [[0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_initialize_parameters(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - The Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Implement `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Look at the mathematical representation of your model $(4)$ in the section [1.2](#1.2):\n",
    "\\begin{align}\n",
    "Z &=  w X + b\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\end{align}\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute `Z` multiplying arrays `w`, `X` and adding vector `b`. Set the prediction array $A$ equal to $Z$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- The output\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    W = parameters['W']\n",
    "    b = parameters['b']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate Z.\n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    Z = np.dot(W,X)+ b\n",
    "    Y_hat = Z\n",
    "    ### END CODE HERE ###\n",
    "    print(Y_hat.shape,(n_y,X.shape[1]))\n",
    "    assert(Y_hat.shape == (n_y, X.shape[1]))\n",
    "\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30) (1, 30)\n",
      "[[ 0.57064241 -1.91914223  1.54789273 -0.30841     2.04748542  0.89877568\n",
      "  -4.11659765 -1.22293512 -0.68693055 -1.57016256 -3.68482633 -1.9685988\n",
      "  -1.96729684  2.02789231  3.12082    -0.21980496 -1.67374385  2.90535037\n",
      "   2.61516789  1.61261083 -1.36151634  0.94860889 -0.94470303 -0.47915225\n",
      "   1.04243989  0.07550471  1.6112966  -0.44603095 -1.09420494 -0.57668459]]\n"
     ]
    }
   ],
   "source": [
    "Y_hat = forward_propagation(X, parameters)\n",
    "\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the elements of the array Y_hat maybe be different depending on the initial parameters. If you would like to get exactly the same output, try to restart the Kernel and rerun the notebook.\n",
    "\n",
    "```Python\n",
    "[[ 0.00570642 -0.01919142  0.01547893 -0.0030841   0.02047485  0.00898776\n",
    "  -0.04116598 -0.01222935 -0.00686931 -0.01570163 -0.03684826 -0.01968599\n",
    "  -0.01967297  0.02027892  0.0312082  -0.00219805 -0.01673744  0.0290535\n",
    "   0.02615168  0.01612611 -0.01361516  0.00948609 -0.00944703 -0.00479152\n",
    "   0.0104244   0.00075505  0.01611297 -0.00446031 -0.01094205 -0.00576685]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 5) (1, 5)\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_forward_propagation(forward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that your weights were just initialized with some random values, so the model has not been trained yet. \n",
    "\n",
    "Define a cost function $(5)$ which will be used to train the model:\n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function as a sum of squares\n",
    "    \n",
    "    Arguments:\n",
    "    Y_hat -- The output of the neural network of shape (n_y, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- sum of squares scaled by 1/(2*number of examples)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Number of examples.\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute the cost function.\n",
    "    cost = np.sum((Y_hat - Y)**2)/(2*m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 507.9383358215691\n"
     ]
    }
   ],
   "source": [
    "print(\"cost = \" + str(compute_cost(Y_hat, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to minimize the cost value, bringing it as close as possible to $0$, making your vector of predictions as similar to the training data as possible.\n",
    "\n",
    "To achieve this, backward propagation needs to be performed. It is covered in details in the Course \"Calculus\" (Course 2 in the Specialization \"Mathematics for Machine Learning\"). For now you can use a function `train_nn()` from the uploaded toolbox to get the updated parameters in each step of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[39.05884267]]\n",
      "b = [[-0.8172973]]\n"
     ]
    }
   ],
   "source": [
    "parameters = w3_tools.train_nn(parameters, Y_hat, X, Y)\n",
    "\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in nn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "Build your neural network model in `nn_model()`.\n",
    "\n",
    "**Instructions**: The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, num_iterations=10, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (n_x, number of examples)\n",
    "    Y -- labels of shape (n_y, number of examples)\n",
    "    num_iterations -- number of iterations in the loop\n",
    "    print_cost -- if True, print the cost every iteration\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[1]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x,n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (~ 2 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"Y_hat\".\n",
    "        Y_hat = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"Y_hat, Y\". Outputs: \"cost\".\n",
    "        cost = np.sum((Y_hat - Y)**2)/(2*m)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        # Parameters update.\n",
    "        parameters = w3_tools.train_nn(parameters, Y_hat, X, Y) \n",
    "        \n",
    "        # Print the cost every iteration.\n",
    "        if print_cost:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30) (1, 30)\n",
      "Cost after iteration 0: 617.205042\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 1: 67.971664\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 2: 37.690211\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 3: 35.497525\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 4: 35.323507\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 5: 35.309358\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 6: 35.308201\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 7: 35.308106\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 8: 35.308098\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 9: 35.308098\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 10: 35.308098\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 11: 35.308098\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 12: 35.308098\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 13: 35.308098\n",
      "(1, 30) (1, 30)\n",
      "Cost after iteration 14: 35.308098\n",
      "W = [[32.24855804]]\n",
      "b = [[1.14465379]]\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X, Y, num_iterations=15, print_cost=True)\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))\n",
    "\n",
    "W_simple = parameters[\"W\"]\n",
    "b_simple = parameters[\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the actual values can be different!\n",
    "\n",
    "```Python\n",
    "Cost after iteration 0: 791.431703\n",
    "Cost after iteration 1: 176.530000\n",
    "Cost after iteration 2: 143.772255\n",
    "Cost after iteration 3: 141.433606\n",
    "Cost after iteration 4: 141.248744\n",
    "Cost after iteration 5: 141.233728\n",
    "Cost after iteration 6: 141.232500\n",
    "Cost after iteration 7: 141.232400\n",
    "Cost after iteration 8: 141.232391\n",
    "Cost after iteration 9: 141.232391\n",
    "Cost after iteration 10: 141.232391\n",
    "Cost after iteration 11: 141.232391\n",
    "Cost after iteration 12: 141.232391\n",
    "Cost after iteration 13: 141.232391\n",
    "Cost after iteration 14: 141.232391\n",
    "W = [[35.71958208]]\n",
    "b = [[2.2893077]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 30) (1, 30)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "(1, 5) (1, 5)\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_nn_model(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that after a few iterations the cost function does not change anymore (the model converges).\n",
    "\n",
    "*Note*: This is a very simple model. In reality the models do not converge that quickly.\n",
    "\n",
    "The final model parameters can be used for making predictions. Let's plot the linear regression line and some predictions. The regression line is red and the predicted points are blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEGCAYAAACQO2mwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkq0lEQVR4nO3deXyU5bn/8c8VFDXihiKimBk8LkdUXEDcexRcW4Xaan+00aoI0WpRi1bwpFatplWpoharDYLaZtyqVnAXXKjWjYCCLO4mEVzAfQmKkOv3xz3xBMiELDPzzEy+79crLzLPTCaX02a+8zz3fV+3uTsiIiLNKYq6ABERyV0KCRERSUkhISIiKSkkREQkJYWEiIiktE7UBaTTFlts4fF4POoyRETyyqxZsz5y9x7N3VdQIRGPx6muro66DBGRvGJmtanu0+UmERFJSSEhIiIpKSRERCQlhYSIiKSkkBARkZQUEiIikpJCQkREUlJIiIjkM3eYNAnuvz8jT6+QEBHJV2+/DYceCiNGQCKRkV+REyFhZpua2d1m9qqZLTSz/cysu5lNM7M3kv9uFnWdIiI5YeVKGD8edt0VZs6EG2+E227LyK/KiZAArgUecff/BnYHFgJjgcfdfQfg8eRtEZGckkgkiMfjFBUVEY/HSWToE/335s2DAw6A0aNh0CBYsABOOw2KMvN2HnlImNnGwA+ASQDuvtzdPwOGArcmH3Yr8OMo6hMRSSWRSFBWVkZtbS3uTm1tLWVlZZkJiuXL4ZJLYK+94K23wpnD/fdD797p/11NWNR7XJvZHkAlsIBwFjELOBtY7O6bNnncp+6+xiUnMysDygBKSkr619am7FMlIpJW8Xic5t5zYrEYNTU16ftFM2fC8OHhLOIXv4BrroEezTZtbRczm+XuA5q7L/IzCUIn2r2AG9x9T+Br2nBpyd0r3X2Auw/okcYXTURkberq6tp0vM3q6+G882DffeHTT2Hq1DBAncX3ulwIiUXAInd/IXn7bkJofGhmvQCS/y6JqD4RkWaVlJS06XibPPkk9OsHV10FI0fC/PlwzDEdf942ijwk3P0D4F0z2yl5aDDh0tNU4KTksZOAKRGUJyKSUkVFBcXFxascKy4upqKiov1P+vnnYSB60KBw+4knwuylTTbpQKXtlyubDo0CEmbWFXgbOIUQYHeZ2alAHXB8hPWJiKyhtLQUgPLycurq6igpKaGiouL74212//1w+unwwQfhMtMll8BqIZRtkQ9cp9OAAQNcO9OJSN5ZuhTOPhtuvx122y2soN5776z9+lwfuBYR6Zzcw1TWnXeGu++Giy+G6uqsBsTaKCREpKBlfbFbay1aBEOGQGkpbL89vPQSXHQRdO2aWzW7e8F89e/f30VEGlVVVXlxcbED338VFxd7VVVVdEWtXOl+443uG23kvsEG7ldf7b5iRaQ1A9We4n1VYxIiUrCyttittd58M0xnfeqpMHtp4kTYbrtVHhJFzRqTEJFOKeOL3VprxQr485/DoPTs2SEcpk9fIyBaqi3rNScpJESkYGV0sVtrvfIK7L8//Pa3cMQRsHBhaO1t1qbaslpzEwoJESlYGVns1oJEAuLx0JA1HnMSP7k7NOSrqYE774R//Qu23jqnal6rVIMV+filgWsRWV1VVZXHYjE3M4/FYhkbAK6qci8udg/zWsNXMV951QHXu3/0UU7W3AgNXIuIZFY8Ds01oY7FwolELtPAtYhIhtXVNf+BO6Lx5rRRSIiIdMRnn8HIkZR483vZRDTenDYKCRGR9poyBfr2hcmTqTj6OYo3WPVsorgYohpvTheFhIgUvLS3uViyBIYNgx//OGwA9MILlN7/cyonGrFYmN0ai0FlZei6kc80cC0iBa1xH+r6+vrvjxUXF1NZWdn2lt6NDfnOPhu+/BIuvBDGjIF1101z1dnV0sC1QkJEClra2ly8+27Y6+Ghh8J2opMmhUtNBUCzm0Sk0+pwm4uGBrjhBthll9Bz6dpr4ZlnCiYg1kYhISIFrUNtLt54Aw45BM44A/bZB+bNg7POgi5d0lxl7lJIiEhBa1ebixUr4MoroV8/mDsXJk+Gxx6DPn0yXG3uUUiISEErLS2lsrKSWCyGmRGLxVoetJ4zJ5w1jBkDRx0FCxbAKaekbMhX6DRwLSIC8O23cNllcPnl0L07TJgAxx3XKcIhLwauzayLmb1kZg8kb3c3s2lm9kby382irlFECtSzz8Iee4SQKC0NZw/HH98pAmJtciYkgLOBhU1ujwUed/cdgMeTt0VE0uerr8KahwMPhPp6eOQRuOUW2HzzqCvLGTkREmbWG/gRcFOTw0OBW5Pf3wr8OMtliUgOSPtq6UbTpsGuu8J118GZZ4aZS0cckZ7nLiDrRF1A0jXA+cBGTY71dPf3Adz9fTPbsrkfNLMyoAyi27lJRDJj9dXStbW1lJWVAbR9tXSjTz6Bc88NZww77QRPPx3OJKRZkZ9JmNnRwBJ3n9Wen3f3Sncf4O4DevTokebqRCRK5eXlq7TTAKivr6e8vLx9T3jPPWER3D/+ARdcAC+/rIBYi8hDAjgAGGJmNcAdwCAzqwI+NLNeAMl/l0RXoohEocOrpRt98EGYqXTccdCrF8ycCX/8I6y/fhqqbJuMXT7LkMhDwt0vcPfe7h4HhgFPuPsJwFTgpOTDTgKmRFSiiESkQ6ulITTku+WWcPbwwAPwpz/Biy/Cnnumr8g2aLx8Vltbi7t/f/ksl4Mi8pBoweXAYWb2BnBY8raIdCLtWi3dqKYGjjwyLITbZZewSG7s2Eg7tqb98lkW5FRIuPtT7n508vuP3X2wu++Q/PeTqOsTkexq82ppCA35/vKXMHPp2WfD9zNmhEHqiKXt8lkWacW1iBSOV1+FESPgP/8J01n/9rew+0+OSFvb8jTLixXXIiLt9t13YSB6993Daulbb4WHH86pgIAOXj6LiEJCRPLbSy/BwIFQXg5Dh8LChfDLX+ZkS412XT6LmEJCRHJayimj33wT1jrsvXeY4nrvvXDXXdCzZ7QFr0VpaSk1NTU0NDRQU1OT0wEBubPiWkRkDalWXPd47TUOv/NOeP11GD4c/vxn2Ew9QDNBISEiOWv1KaPdgD/V13P4pZdCPB76Lx16aGT1dQYKCRHJWU2nhh4B/A3YFrgWOPuVV6Bbt4gq6zw0JiEiOaukpITNgFuAR4CvCX18xsdiCogsUUiISG5y5+9Dh/Iq8AvgUmBPYG6OTxktNAoJEck9778PP/0pP7juOrr06cOQXr24yIxeeTBltNBoTEJEckdjQ77Ro8MU1yuuYPPRo3l4Hb1VRUVnEiKSG955Bw4/HIYPZ0nPnhzSvTtFY8cS3377nO6SWugUEiISrZUrwxaiu+4Kzz/PiyefzHZ1dTz13nt50067kCkkRCQ6CxfCQQfB2WfD//wPzJ/Pz558kq+XLVvlYbneTruQKSREJPu++w4qKmCPPcKq6aoqePBBKCnJy3bahUwhISLZNWsWDBgAv/sdHHts6NpaWvp9Q74O70YnaaWQEJHsWLYMxoyBffaBpUvhvvvgjjtgyy1XeVg+ttMuZAoJEcm8f/877PVw5ZVhO9EFC0Jb72bkYzvtQqad6UQkc774IuwrfcMN0KcPTJwIgwdHXZWsRjvTiUj2PfRQmNZ6443wm9/AK68oIPJQ5CFhZtua2ZNmttDM5pvZ2cnj3c1smpm9kfxXzeJF8sFHH8GJJ8KPfgQbbQTPPgtXXw0bbhh1ZdIOkYcEsAI41913BvYFzjSzvsBY4HF33wF4PHlbRHKVO9x5J/TtGwakf/97mD0b9t036sqkAyIPCXd/391nJ7//ElgIbAMMBW5NPuxW4MeRFCgia3XvhAlM23BDGDaMOZ9/zoN/+ANccgmst17UpUkHRR4STZlZnNAN+AWgp7u/DyFIgC1T/EyZmVWbWfXSpUuzVquIAO48P2IEg0aN4sBlyzgP6L98OT+77DK10SgQORMSZtYNuAc4x92/aO3PuXuluw9w9wE9evTIXIEisqq33oLBg9l30iReAnYDrgJWojYahSQnQsLM1iUERMLd700e/tDMeiXv7wUsiao+EWli5cowEL3bbjBrFqcBg4G3VnuY2mgUhshDwswMmAQsdPerm9w1FTgp+f1JwJRs1yYiq5k3D/bfH849N0xnnT+fR2MxmlttpTYahSHykCBsWXsiMMjMXk5+/RC4HDjMzN4ADkveFpEoLF8eBqL32gvefhtuvx2mToXevdVGo8BFvt2Tuz8DWIq7tfJGJGovvginnhrOIn7xC7j2Wthii+/vbmyXUV5eTl1dHSUlJVRUVKiNRoFQWw4RaV59fVjrMH489OoVVk4ffXTUVUkGtNSWI/IzCRHJQU8+CSNGhEtLp58OV1wBG28cdVUSgVwYkxCRXPH553DaaTBoEBQVwVNPheZ8CohOSyEhIsH994eWGjfdBOedB3PmhC1FpVNTSIh0dkuXws9/DkOGwOabwwsvwLhxsNqMJemcFBIiEUgkEsTjcYqKiojH49G0sHCH226DnXeGe+6BP/wBqqvD1qIiSQoJkSxLJBKUlZVRW1uLu1NbW0tZWVmLQZH2UFm0CI45Juwtvf328NJLcOGF0LVrx55XCo6mwIpkWTwep7a2do3jsViMmpqaNY43hkp9ff33x4qLi9u3pWdDQ9gd7re/De01Kipg1Cjo0qWt/xlSQFqaAquQEMmyoqIimvu7MzMaGhrWON7WUEnpjTdg5EiYMSPMXpo4Ebbbri2lS4HS9qUiOSRVT6NUx1M1ymupgV4iAfF4mMUajzmJXzwA/frByy+H2UvTpysgpFUUEiJZ1tZeR20NlUQCysqgtjaMTdfWGWW3H0Ki72WwYEFosWGpOuG0rHFsxMxYZ511MLPoBt4lO9y9YL769+/vIvmgqqrKY7GYm5nHYjGvqqpq8bHFxcUOfP9VXFyc8mdiMfcQD6t+xUoaOlzz6nW0ph7JfUC1p3hf1ZiESB5IJBKtbqBXVOS4r3mmYBbGrdsr1dhIozaPkUjO0MC1SGfw9ddw4YXEx59FLfE17o7FoCPv4akG3BulGniX3KeBa5FC9/jjYae48eOpOPRJijdY9c28uDjMdu2ItW0ipE2GCpNCQiSfffZZ6NZ66KGwzjowYwal006hcqIRi4VLTLEYVFaGdXMd0dyAeyNtMlS4FBIi+WrKlNCQ75ZbYMyY0JDvBz8AQiDU1IQxiJqaNQOiPSu4S0tLqaysJBaLAdAluQAvFou1b2Gf5AWNSYjkmw8/hLPOgrvuCmsfJk+G/v1b/eNpXcEtBUFjEiI5oMP9l9yhqiqcPdx3H1x2WWjI14aAgLDNaNOAAKivr6e8vLxt9UinoJ3pRLJg9U/vjU39gNZ9eq+rCzvEPfww7LcfTJoUure2Q3tWcEvnlfNnEmZ2pJm9ZmZvmtnYqOsRaY92f3pvaIC//hV22SX0XLr2Wnj66XYHBLR9Bbd0bjkdEmbWBbgeOAroC/zczPpGW5VI27Xr0/vrr8PBB8OZZ4azh3nzwlhEBzu2trUtiHRuOR0SwEDgTXd/292XA3cAQyOuSaTN2vTpfcUKuOKKMCj9yivh0tKjj0KfPmmppeksJTPT7CRpUa6HxDbAu01uL0oe+56ZlZlZtZlVL126NKvFibRWqz+9z5kD++wDY8fCD38YGvINH97uhnyplJaWUlNTQ0NDAzU1NQoISSnXQ6K5v4xV5uy6e6W7D3D3AT169MhSWSJts9ZP7998A7/7Xdg6dPFiuPtuuPde6NUr2sKl08v12U2LgG2b3O4NvBdRLSIdUlpa2vwn9mefDe27X30VTjoJrr4aunfPfoEizcj1M4mZwA5m1sfMugLDgKkR1yTSolavh/jqqzAQfeCBUF8PjzwSVk8rICSH5PSZhLuvMLNfA48CXYDJ7j4/4rJEUmr1eojHHgs7A9XVhdlLf/wjbLRRFCWLtEhtOUTSaK37UX/yCZx7bjhj2GmnsJXogQdmvU6RptSWQyRLWlwPcc89oaXGP/4BF1wQ9ptWQEiOW2tImNl0M9s9G8WI5Lvm1j30BB7YYAM47rgwW2nmzHB5af31s1+gSBu15kzifGC8md1sZpqPJ9KC1ddDnAQsBI747rsQDC++CHvuGVl9Im211oFrd58NDDKznwKPmNm9wJXuvizj1YnkmcbB6RvGjOH3ixdzOLBkxx3pMnVqGIMQyTOtGpMwMwNeA24ARgFvmNmJmSxMJC81NFD6ySc889lnHN6tG0yYwJYLFyogJG+1ZkziGWAxMJ7QEuNk4GBgoJlVZrI4kbyycCEcdFBY+3DQQaEh35lnQpHmh0j+as06idOB+b7mXNlRZrYwAzWJ5JfvvoNx4+CSS6BbN/j73+GEE9Leb0kkCmv9iOPu85oJiEY/SnM9IhnT4Z3hmjN7NgwcCOXlMGRIaMh34okKCCkYHToPdve301WISCY1roSura3F3b9fCd3uoFi2LKx1GDgQPvggNOP75z+hZ8/0Fi4SMV0slU4hHfs6N56JHGTGWxtvDJdfDiefHM4ejj02zRWL5AaFhHQKHd3XOZFIMHrkSM6rreVpoGjFCo5ebz0ShxwCm22WxkpFcotCQjqFju7r/Njo0cxctowzCNP8dgUe/PbbNp2JiOQjhYR0Cu3e1/njj+GXv+TWJUv4CjgAGA00Xrhq7ZmISL5SSEin0OZ9nd3DQHTfvnD77Vy7ySbsCTy/2sO6d++e/hlTIjlEISGdRqp9nVefGnvPhAnwk5/Az34G224L1dVscf31rLPamci6667Ll19+mb4ZUyI5SCEhBam1ayJWnxo7qLaWwaNGseLBB+HKK+H552H33Zs9E9l4441Zvnz5Ks/X1hlTIrlOmw5JwVl9dzgI4w/NXV5q3CSoD/A34DBgBnDx1lvz5OLFLf6eoqIimvv7MTMaGho6/h8ikiXadEg6lbasiVhUW8vZwCvAPoQeNIcAM95/f62/p6MzpkTygUJCCk6r10QsWMCLXbtyDfAUsAvhbMJp3Rt9u2dMieQRhYQUnLV+wl++HC69FPbck75duzK8a1eOBhYlH9faN/o2z5gSyUORhoSZjTOzV81srpn9y8w2bXLfBWb2ppm9ZmZHRFim5JkWP+FXV8Pee8Pvfw8/+Qnrv/UWgydPbvcbfaoZUyKFIuoziWnAru7eD3gduADAzPoCwwhXAI4E/mpmXSKrUrImHZ1am/uEP2nCBErnzIF99oGPPoIpU+D222HLLdP6Rp+RTrMiUXL3nPgCjgUSye8vAC5oct+jwH5re47+/fu75K+qqiovLi52wrCAA15cXOxVVVUde+KnnnLffnt3cB850v3TT9NS7+oyVr9IhgHVnuJ9NeoziaaGAw8nv98GeLfJfYuSx9ZgZmVmVm1m1UuXLs1wiZJJ6ejUuoovvoBf/QoOPhgaGuDxx6GyEjbdtMO1Nift9YvkgIyHhJlNN7N5zXwNbfKYcmAF0Hhu3tyOLc0u6HD3Sncf4O4DevTokf7/AMmajnZqXcVDD8Euu4RQGD0a5s6FQYM6WGHL0lq/SI5ozfalHeLuh7Z0v5mdBBwNDE6e9kA4c9i2ycN6A+9lpkLJFSUlJdTW1jZ7vNU++gjOOQcSiRASd98dxiGyIC31i+SYqGc3HQmMAYa4e9Pz9KnAMDNbz8z6ADsAL0ZRo2RPh9YduMOdd4aGfHfdBRddFLYWzVJAgNZNSIFKNViRjS/gTcLYw8vJrxub3FcOvAW8BhzVmufTwHX+q6qq8lgs5mbmsVisdYO+ixa5DxkSBqb33tt97tzMF5pCu+oXiRgtDFyrd5PktEQiQXl5OXV1dZSUlFBRUfF/U1Td4aab4Lzz4LvvwgK5c86BLpotLdIWLfVuyviYhEh7rd6or7EVN0DpvvvCyJHw5JNh9tLEibD99hFWK1KYFBKSs5qbUvpNfT1vjxoF33wD664Lf/sbjBgBRbk0m1ukcOgvSzKqIyuQV586ugvwLHDhp5/C4MEwfz6UlSkgRDJIf12SMatv6NPWndsap46uC1wEzAa2A0ZtsQVMnQq9e2eqdBFJUkhIxnR0BXJFRQUHrbces4GLgX8C/TfYgH2vuQasufWWIpJuCgnJmLasQE4kIB4PV47icUjc/C2ls2fz1PLlbN6lC8cA5bEYf5o4UZ1WRbJIA9eSMa1dgZxIhKGFxpOO2looO3Ul+PuUlo2k15VXcv8mm2SjZBFZjc4kJGNauwK5vPz/AqJRvRdT3nNymL2kgBCJjEJCMqa1O7el6n9Xt2T9LFQpIi3RimuJ1tKlxONQW79mB99YDGpqsl6RSKfT0oprnUlINNzDYMTOO1PxzWiK112+yt3FxdCavnjaCU4ksxQSkn3vvgvHHAMnnAA77EDp3LFU3tyVWCzMbI3FwjYQa5vE1NF1GCKydrrcJNnT0BDe/c8/H1auhD/+EX7963Y35IvH483OnorFYtToOpVIq6nBn0TvjTdCQ74ZM0JLjcpK2G67Dj2ldoITyTxdbpLMWrECxo2Dfv3g5Zdh0iSYNq3DAQGpd3zTTnAi6aOQkMyZO5ePd9wRzj+f+775hoHdupFYb720tdTQTnAimaeQkPT79lv4/e9p2GsvVr7zDj8DjgVmLl6c1oHl1q7DEJH208C1pNdzz8Gpp8LChdyz4YaUff01n6z2EA0si+QWrZOQzPv667B16AEHwFdfwUMPcXx9/RoBARpYFsknCgnpuOnTYddd4dpr4YwzwmZARx2lgWWRApATIWFm55mZm9kWTY5dYGZvmtlrZnZElPVJCp99Fi4tHXZY2Er03/+GCRNgo40ADSyLFILIQ8LMtgUOA+qaHOsLDCPsWHkk8Fcza9+KK8mM++6Dvn3h1lth7FiYMwcOOmiVh2hgWST/5cJiuvHA+cCUJseGAne4+7fAO2b2JjAQeC6C+qSpDz+EUaPgn/+E3XeH+++H/v1TPry0tFShIJLHIj2TMLMhwGJ3n7PaXdsA7za5vSh5TKLiDn//O+y8M0yZApddBjNnthgQIpL/Mn4mYWbTga2auasc+F/g8OZ+rJljzc7VNbMyoAw0IJoxdXVw2mnwyCOw//5h1fR//3fUVYlIFmQ8JNz90OaOm9luQB9gjoUVuL2B2WY2kHDmsG2Th/cG3kvx/JVAJYR1EumrXGhogBtuCGMO7mH20plntrshn4jkn8jGJNz9FWDLxttmVgMMcPePzGwqcJuZXQ1sDewAvBhJoZ3Va6/BiBHwzDNh9lJlJcTjUVclIlkW+eym5rj7fOAuYAHwCHCmu6+MtqpOYsUKuPzyMCg9bx7cfDM8+qgCQqSTyoXZTQC4e3y12xWAJtRn08svh3UPs2fDT38a1jxs1dxwkoh0Fjl5JiFZ9s03UF4OAwbA4sVw993hSwEh0unlzJmEROQ//wljD6++CiefDFddBd27R12ViOQInUl0Vl99BWedFVZJL1sWxh1uvlkBISKrUEh0Ro89FhryTZgQ9pieNw8Ob265ioh0dgqJzuSTT+CUU+CII2D99eHpp+G666Bbt4z/6kQiQTwep6ioiHg8nraNh0QkszQm0Vncc09YCPfRR/C//wsXXhiCIgsSiQRlZWXU19cDUFtbS1lZGYD6OonkOJ1JFLoPPgjTWY87DrbeGqqroaIiawEBUF5e/n1ANKqvr6e8vDxrNYhI+ygkCpU73HJLaMj34INhgdwLL8Aee2S9lFQ70WmHOpHcp5AoRDU1YdzhlFPCAPWcOTBmTNgYKALaoU4kfykkCklDA/zlLyEYnnsOrr8eZsyAnXaKtCztUCeSvxQShWLhwrDmoXHtw7x5Yb/pouj/J9YOdSL5y9wLp7v2gAEDvLq6Ouoysuu772DcOLjkkjCV9Zpr4IQTwJrbkkNEZE1mNsvdBzR3n6bA5rPZs2H48DDmcPzx4VJTz55RVyUiBST6axHSdsuWhY2ABg4Me07/619w110KCBFJO51J5Junnw4N+V5/PbT1HjcONtss6qpEpEDpTCJffPllWDH9gx/A8uUwbRrcdJMCQkQySiGRDx5+GHbZJew3fc45YebSoc1uHS4iklYKiVz28cfwy1/CD38YZi795z8wfjxsuGHUlYlIJ6GQyEXuYSB6553h9ttDM76XXoL99ou6MhHpZDRwnWveey+MPdx3H/TvD9OnQ79+UVclIp2UziRyhTtMmgR9+8Ijj8CVV8LzzysgRCRSkYeEmY0ys9fMbL6ZXdnk+AVm9mbyviOirDHj3n4bDjssTG3dfXeYOxd++1tYRyd6IhKtSN+FzOwQYCjQz92/NbMtk8f7AsOAXYCtgelmtqO7r4yu2gxYuTKski4vhy5d4MYbYeTInOi3JCIC0Z9J/Aq43N2/BXD3JcnjQ4E73P1bd38HeBMYGFGNmbFgARx4IPzmN3DwwTB/Ppx2mgJCRHJK1O9IOwIHmdkLZjbDzPZOHt8GeLfJ4xYlj63BzMrMrNrMqpcuXZrhctNg+XK49NKw+c8bb0AiAQ88ANtuG3VlIiJryPjlJjObDmzVzF3lyd+/GbAvsDdwl5ltBzTXwrTZdrXuXglUQugCm46aM2bmzNBK45VXYNgwuO466NEj6qpERFLKeEi4e8qlwWb2K+BeD/3KXzSzBmALwplD04/WvYH3MlpoJtXXw8UXw1VXwVZbwZQpMGRI1FWJiKxV1Jeb7gMGAZjZjkBX4CNgKjDMzNYzsz7ADsCLURXZITNmhBlL48aF2UsLFiggRCRvRD3HcjIw2czmAcuBk5JnFfPN7C5gAbACODPvZjZ98UXYV/rGG+G//gueeAIOOSTqqkRE2iTSkHD35cAJKe6rAPJzE+QHH4TTTw+rp0ePDgPVq+3xLCKSD6K+3FRYli6F0lI4+mjYdFN47rkwDqGAEJE8pZBIB3ee+fWv+XirrVh+222M32QTbj/vvLBznIhIHot6TCL/LV7MomOO4cCXXuIF4FRg/uefU3zGGTSssw6lpaVRVygi0m46k2gvd5g4Efr2ZfOXX2Y0sD8wP3l3fX095eXlERYoItJxCon2eOstGDwYyspgr73o5854oGG1h9XV1UVRnYhI2igk2mLlSrj6athtN5g1Cyor4Ykn+C4Wa/bhJSUlWS5QRCS9FBKtNW8e7L8/nHtu2F96wYLQsdWMiooKilebwVRcXExFRX7O4BURaaSQWJvly+GSS2CvveCdd+COO0JbjW3+r99gaWkplZWVxGIxzIxYLEZlZaUGrUUk71lY4FwYBgwY4NXV1el7whdfDA355s0L6x+uuQa22CJ9zy8ikgPMbJa7D2juPp1JAIlEgng8TlFREfF4nDsmTw6XlfbbDz77LLTyrqpSQIhIp9Pp10kkEgnKysqor68HYLvaWgaOGBGmuJ5+OlxxBWy8ccRViohEo9OfSZSXl1NfX88mhE0pngBWuvP/evaEG25QQIhIp9bpQ6Kuro7+hEVww4ErgH7AP5csafHnREQ6g04fEiUlJbxNCIl9gLHAN2iNg4gIKCSoqKjg2+JijgBmJY9pjYOISNDpQ0JrHEREUtM6CRGRTk7rJEREpF0UEiIikpJCQkREUlJIiIhISgoJERFJSSEhIiIpFdQUWDNbCtRGXcdqtgA+irqIHKfXqGV6fdZOr1HL1vb6xNy9R3N3FFRI5CIzq041/1gCvUYt0+uzdnqNWtaR10eXm0REJCWFhIiIpKSQyLzKqAvIA3qNWqbXZ+30GrWs3a+PxiRERCQlnUmIiEhKCgkREUlJIZEFZjbOzF41s7lm9i8z2zTqmnKNmR1vZvPNrMHMNJUxycyONLPXzOxNMxsbdT25xswmm9kSM5sXdS25yMy2NbMnzWxh8u/r7LY+h0IiO6YBu7p7P+B14IKI68lF84CfAP+OupBcYWZdgOuBo4C+wM/NrG+0VeWcW4Ajoy4ih60AznX3nYF9gTPb+v8hhUQWuPtj7r4iefN5oHeU9eQid1/o7q9FXUeOGQi86e5vu/ty4A5gaMQ15RR3/zfwSdR15Cp3f9/dZye//xJYCGzTludQSGTfcODhqIuQvLAN8G6T24to4x+4SCMziwN7Ai+05efWyUg1nZCZTQe2auaucnefknxMOeH0L5HN2nJFa14jWYU1c0xz1qXNzKwbcA9wjrt/0ZafVUikibsf2tL9ZnYScDQw2Dvp4pS1vUayhkXAtk1u9wbei6gWyVNmti4hIBLufm9bf16Xm7LAzI4ExgBD3L0+6nokb8wEdjCzPmbWFRgGTI24JskjZmbAJGChu1/dnudQSGTHBGAjYJqZvWxmN0ZdUK4xs2PNbBGwH/CgmT0adU1RS052+DXwKGHA8S53nx9tVbnFzG4HngN2MrNFZnZq1DXlmAOAE4FByfeel83sh215ArXlEBGRlHQmISIiKSkkREQkJYWEiIikpJAQEZGUFBIiIpKSQkJERFJSSIiISEoKCZEMS/bzPyz5/WVmdl3UNYm0lno3iWTeRcAfzGxLQhfOIRHXI9JqWnEtkgVmNgPoBhyc7Osvkhd0uUkkw8xsN6AX8K0CQvKNQkIkg8ysF2H/kKHA12Z2RMQlibSJQkIkQ8ysGLiXsMfwQuBS4OJIixJpI41JiIhISjqTEBGRlBQSIiKSkkJCRERSUkiIiEhKCgkREUlJISEiIikpJEREJKX/Dyq1m+4R0JfmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_pred = np.array([-0.95, 0.2, 1.5])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X, Y, color = \"black\")\n",
    "\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "    \n",
    "X_line = np.arange(np.min(X[0,:]),np.max(X[0,:])*1.1, 0.1)\n",
    "ax.plot(X_line, W_simple[0,0] * X_line + b_simple[0,0], \"r\")\n",
    "ax.plot(X_pred, W_simple[0,0] * X_pred + b_simple[0,0], \"bo\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for such a small neural network with just a single perceptron and one input node!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Multiple Linear Regression\n",
    "\n",
    "Models are not always as simple as the one above. In some cases your output is dependent on more than just one variable. Let's look at the case where the output depends on two input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Multipe Linear Regression Model\n",
    "\n",
    "Multiple linear regression model with two independent variables $x_1$, $x_2$ can be written as\n",
    "\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + b = Wx + b,\\tag{6}$$\n",
    "\n",
    "where $Wx$ is the dot product of the input vector $x = \\begin{bmatrix} x_1 & x_2\\end{bmatrix}$ and parameters vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$, scalar parameter $b$ is the intercept. \n",
    "\n",
    "The goal is the same - find the \"best\" parameters $w_1$, $w_2$ and $b$ such the differences between original values $y_i$ and predicted values $\\hat{y}_i$ are minimum.\n",
    "\n",
    "You can use a slightly more complicated neural network model to do that. Now matrix multiplication will be in the core of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Neural Network Model with a Single Perceptron and Two Input Nodes\n",
    "\n",
    "Again, you will use only one perceptron, but with two input nodes shown in the following scheme:\n",
    "\n",
    "<img src=\"images/nn_model_linear_regression_multiple.png\" style=\"width:420px;\">\n",
    "\n",
    "The perceptron output calculation for every training example $x^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)}\\end{bmatrix}$ can be written with dot product:\n",
    "\n",
    "$$z^{(i)} = w_1x_1^{(i)} + w_2x_2^{(i)} + b = Wx^{(i)} + b,\\tag{7}$$\n",
    "\n",
    "where weights are in the vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$ and bias $b$ is a scalar. The output layer will have the same single node $\\hat{y}^{(i)} = z^{(i)}$.\n",
    "\n",
    "Organise all training examples in a matrix $X$ of a shape ($2 \\times m$), putting $x_1^{(i)}$ and $x_2^{(i)}$ into columns. Then matrix multiplication of $W$ ($1 \\times 2$) and $X$ ($2 \\times m$) will give a ($1 \\times m$) vector\n",
    "\n",
    "$$WX = \n",
    "\\begin{bmatrix} w_1 & w_2\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "x_1^{(1)} & x_1^{(2)} & \\dots & x_1^{(m)} \\\\ \n",
    "x_2^{(1)} & x_2^{(2)} & \\dots & x_2^{(m)} \\\\ \\end{bmatrix}\n",
    "=\\begin{bmatrix} \n",
    "w_1x_1^{(1)} + w_2x_2^{(1)} & \n",
    "w_1x_1^{(2)} + w_2x_2^{(2)} & \\dots & \n",
    "w_1x_1^{(m)} + w_2x_2^{(m)}\\end{bmatrix}.$$\n",
    "\n",
    "And the model can be written as\n",
    "\n",
    "\\begin{align}\n",
    "Z &=  W X + b,\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\tag{8}\\end{align}\n",
    "\n",
    "where $b$ is broadcasted to the vector of a size ($1 \\times m$). These are the calculations to perform in the forward propagation step. Cost function will remain the same, and there will be no change in methodology and training (that will be discussed in the next Course)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Dataset\n",
    "\n",
    "Let's build a linear regression model for a Kaggle dataset [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), saved in a file `data/house_prices_train.csv`. You will use two fields - ground living area (`GrLivArea`, square feet) and rates of the overall quality of material and finish (`OverallQual`, 1-10) to predict sales price (`SalePrice`, dollars).\n",
    "\n",
    "To open the dataset you can use `pandas` function `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/house_prices_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now saved in the variable `df` as a **DataFrame**, which is the most commonly used `pandas` object. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it as a table or a spreadsheet. Full documentation can be found [here](https://pandas.pydata.org/).\n",
    "\n",
    "Select the required fields and save them in the variables `X_multi`, `Y_multi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X_multi = df[['GrLivArea', 'OverallQual']]\n",
    "Y_multi = df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_multi:\n",
      "      GrLivArea  OverallQual\n",
      "0          1710            7\n",
      "1          1262            6\n",
      "2          1786            7\n",
      "3          1717            7\n",
      "4          2198            8\n",
      "...         ...          ...\n",
      "1455       1647            6\n",
      "1456       2073            6\n",
      "1457       2340            7\n",
      "1458       1078            5\n",
      "1459       1256            5\n",
      "\n",
      "[1460 rows x 2 columns]\n",
      "\n",
      "Y_multi:\n",
      "0       208500\n",
      "1       181500\n",
      "2       223500\n",
      "3       140000\n",
      "4       250000\n",
      "         ...  \n",
      "1455    175000\n",
      "1456    210000\n",
      "1457    266500\n",
      "1458    142125\n",
      "1459    147500\n",
      "Name: SalePrice, Length: 1460, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_multi:\\n{X_multi}\\n\")\n",
    "print(f\"Y_multi:\\n{Y_multi}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the original arrays have different units. To make training of the neural network efficient, you need to bring them to the same units. A common approach to it is called **normalization**: substract the mean value of the array from each of the elements in the array and divide them by standard deviation (a statistical measure of the amount of dispersion of a set of values). If you are not familiar with mean and standard deviation, do not worry about this for now - this is covered in the third Course of Specialization.\n",
    "\n",
    "Normalization is implemented in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X_multi_norm = (X_multi - np.mean(X_multi))/np.std(X_multi)\n",
    "Y_multi_norm = (Y_multi - np.mean(Y_multi))/np.std(Y_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert results to the `NumPy` arrays, transpose `X_multi_norm` to get an array of a shape ($2 \\times m$) and reshape `Y_multi_norm` to bring it to the shape ($1 \\times m$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X: (2, 1460)\n",
      "The shape of Y: (1, 1460)\n",
      "I have m = 1460 training examples!\n"
     ]
    }
   ],
   "source": [
    "X_multi_norm = np.array(X_multi_norm).T\n",
    "Y_multi_norm = np.array(Y_multi_norm).reshape((1, len(Y_multi_norm)))\n",
    "\n",
    "print ('The shape of X: ' + str(X_multi_norm.shape))\n",
    "print ('The shape of Y: ' + str(Y_multi_norm.shape))\n",
    "print ('I have m = %d training examples!' % (X_multi_norm.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Performance of the Neural Network Model for Multiple Linear Regression\n",
    "\n",
    "The magic is that now you do not need to change anything in your neural network implementation! Go through the code in section [2](#2) and see that if you pass new datasets `X_multi_norm` and `Y_multi_norm`, the input layer size $n_x$ will get equal to $2$ and the rest of the implementation will remain exactly the same. That's the power of the neural networks (and matrix multiplication)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex06'></a>\n",
    "### Exercise 6\n",
    "\n",
    "Run the constructed above neural network model `nn_model()` for `100` iterations, passing the training dataset saved in the arrays `X_multi_norm` and `Y_multi_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 0: 75.453980\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 1: 57.847342\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 2: 47.669171\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 3: 40.376470\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 4: 34.621118\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 5: 29.918103\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 6: 26.030655\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 7: 22.805537\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 8: 20.126797\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 9: 17.901056\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 10: 16.051495\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 11: 14.514479\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 12: 13.237177\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 13: 12.175702\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 14: 11.293584\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 15: 10.560517\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 16: 9.951316\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 17: 9.445051\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 18: 9.024329\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 19: 8.674696\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 20: 8.384140\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 21: 8.142680\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 22: 7.942018\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 23: 7.775263\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 24: 7.636684\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 25: 7.521520\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 26: 7.425816\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 27: 7.346282\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 28: 7.280188\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 29: 7.225261\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 30: 7.179615\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 31: 7.141682\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 32: 7.110159\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 33: 7.083961\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 34: 7.062191\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 35: 7.044099\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 36: 7.029064\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 37: 7.016569\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 38: 7.006186\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 39: 6.997557\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 40: 6.990386\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 41: 6.984427\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 42: 6.979475\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 43: 6.975359\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 44: 6.971939\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 45: 6.969097\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 46: 6.966735\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 47: 6.964772\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 48: 6.963141\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 49: 6.961785\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 50: 6.960659\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 51: 6.959723\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 52: 6.958945\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 53: 6.958298\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 54: 6.957761\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 55: 6.957314\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 56: 6.956943\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 57: 6.956635\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 58: 6.956379\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 59: 6.956166\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 60: 6.955989\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 61: 6.955842\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 62: 6.955719\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 63: 6.955618\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 64: 6.955533\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 65: 6.955463\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 66: 6.955405\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 67: 6.955356\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 68: 6.955316\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 69: 6.955283\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 70: 6.955255\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 71: 6.955232\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 72: 6.955213\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 73: 6.955197\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 74: 6.955183\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 75: 6.955172\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 76: 6.955163\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 77: 6.955156\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 78: 6.955149\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 79: 6.955144\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 80: 6.955140\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 81: 6.955136\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 82: 6.955133\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 83: 6.955131\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 84: 6.955128\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 85: 6.955127\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 86: 6.955125\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 87: 6.955124\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 88: 6.955123\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 89: 6.955122\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 90: 6.955122\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 91: 6.955121\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 92: 6.955121\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 93: 6.955120\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 94: 6.955120\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 95: 6.955120\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 96: 6.955119\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 97: 6.955119\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 98: 6.955119\n",
      "(1, 1460) (1, 1460)\n",
      "Cost after iteration 99: 6.955119\n",
      "W = [[0.36942359 0.57177893]]\n",
      "b = [[9.63612751e-17]]\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (~ 1 line of code)\n",
    "parameters_multi = nn_model(X_multi_norm, Y_multi_norm, num_iterations=100, print_cost=True)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"W = \" + str(parameters_multi[\"W\"]))\n",
    "print(\"b = \" + str(parameters_multi[\"b\"]))\n",
    "\n",
    "W_multi = parameters_multi[\"W\"]\n",
    "b_multi = parameters_multi[\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the actual values can be different!\n",
    "\n",
    "```Python\n",
    "Cost after iteration 0: 0.489797\n",
    "Cost after iteration 1: 0.429192\n",
    "Cost after iteration 2: 0.380299\n",
    "Cost after iteration 3: 0.340051\n",
    "Cost after iteration 4: 0.306705\n",
    "Cost after iteration 5: 0.279020\n",
    "...\n",
    "Cost after iteration 95: 0.142913\n",
    "Cost after iteration 96: 0.142913\n",
    "Cost after iteration 97: 0.142913\n",
    "Cost after iteration 98: 0.142913\n",
    "Cost after iteration 99: 0.142913\n",
    "W = [[0.36946186 0.5718172 ]]\n",
    "b = [[1.35781797e-16]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "(1, 1460) (1, 1460)\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_multi(nn_model, X_multi_norm, Y_multi_norm, parameters_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that the initial datasets were normalized. To make the predictions, you need to normalize the original, calculate predictions with the obtained linear regression coefficients and then **denormalize** the result (perform the reverse process of normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground living area, square feet:\n",
      "[1710 1200 2200]\n",
      "Rates of the overall quality of material and finish, 1-10:\n",
      "[7 6 8]\n",
      "Predictions of sales price, $:\n",
      "[[221368. 160041. 281579.]]\n"
     ]
    }
   ],
   "source": [
    "X_pred_multi = np.array([[1710, 7], [1200, 6], [2200, 8]]).T\n",
    "\n",
    "# Normalize using the same mean and standard deviation of the original training array X_multi.\n",
    "X_multi_mean = np.array(np.mean(X_multi)).reshape((2,1))\n",
    "X_multi_std = np.array(np.std(X_multi)).reshape((2,1))\n",
    "X_pred_multi_norm = (X_pred_multi - X_multi_mean)/ X_multi_std\n",
    "# Make predictions.\n",
    "Y_pred_multi_norm = np.matmul(W_multi, X_pred_multi_norm) + b_multi\n",
    "# Denormalize using the same mean and standard deviation of the original training array Y_multi.\n",
    "Y_pred_multi = Y_pred_multi_norm * np.std(Y_multi) + np.mean(Y_multi)\n",
    "\n",
    "print(f\"Ground living area, square feet:\\n{X_pred_multi[0]}\")\n",
    "print(f\"Rates of the overall quality of material and finish, 1-10:\\n{X_pred_multi[1]}\")\n",
    "print(f\"Predictions of sales price, $:\\n{np.round(Y_pred_multi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this programming assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grade-up-to-here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W1_Assignment_Solution.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
