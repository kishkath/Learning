{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8ekBMWgqsZj"
   },
   "source": [
    "# ALL about Activation function\n",
    "\n",
    "\n",
    "<img align=\"left\" width=\"300\" src=\"https://t3.ftcdn.net/jpg/00/88/78/98/360_F_88789804_PHuRoK7HmutsWwwjgQHYqqg2GCYhIHpY.jpg\">\n",
    "\n",
    "<img align=\"left\" width=\"200\"  src=\"https://storage.needpix.com/rsynced_images/emoji-2304720_1280.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SNAPJwJsgkc"
   },
   "source": [
    "# Table of contents\n",
    "1. What is activation Function?\n",
    "2. Activation function Types\n",
    "    1. Linear function\n",
    "    2. Binary Step function\n",
    "    3. Non-Linear function  \n",
    "        1. Sigmoid Function\n",
    "        2. Tanh (Hyperbolic Tangent)\n",
    "        3. Relu (ReLu — Rectified Linear Units)\n",
    "        4. Leaky Relu\n",
    "        5. Elu (Exponential Linear Units)\n",
    "        6. PRelu (Parametric Rectified Linear Units)\n",
    "        7. Swish \n",
    "        8. Softplus\n",
    "        9. Softmax\n",
    "4. Assignment summary\n",
    "5. Feedback Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-9JFqwmeAo0X"
   },
   "outputs": [],
   "source": [
    "# import numpy library\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BSOMpfPvGpa"
   },
   "source": [
    "# What Is Activation Function?\n",
    "\n",
    "An activation function is a very important feature of an artificial neural network , they basically decide whether the neuron should be activated or not.\n",
    "\n",
    "In artificial neural networks, the activation function defines the output of that node given an input or set of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEzV1G6AvyAw"
   },
   "source": [
    "## Important\n",
    "\n",
    "Use of any activation function is to introduce non-linear properties to our Network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy7zH9TgwU2f"
   },
   "source": [
    "![iIcbq.gif](data:image/gif;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zn8pvECwdW-"
   },
   "source": [
    "### From above image:\n",
    "It calculates a \"sum of weights\" of its input and then adds a bias to it then decides whether the neuron should be \"fired\" or not.\n",
    "\n",
    "All inputs X1,X2, X3... Xn are multiplied with the weights W1, W2, W3 ... Wn assigned to each link between inputand node and then summed together along with Bias b\n",
    "\n",
    "y = summation((Wi*Xi) + b)\n",
    "\n",
    "## Note:\n",
    "Xi’s and Wi’s are vectors and b is scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBcdlluZyFvx"
   },
   "source": [
    "The value of Y can be anything ranging from -inf to +inf. Meaning it has lot of information ,now neuron must know to distinguish between the “useful” and “not -so-useful” information.To build this sense into our network we add 'activation function (f)'.\n",
    "\n",
    "The activation Function will decide whether the information passed is useful or not based on the result it get fired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyTqJawsy51-"
   },
   "source": [
    "![link ima](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24112aa6bf00ee6dc2642_Group%20805.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz_1CymQz78r"
   },
   "source": [
    "# Activation function Types :\n",
    "\n",
    "- Linear function\n",
    "- Binary Step function\n",
    "- Non-Linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJTcYljH0kJP"
   },
   "source": [
    "## Linear Function:\n",
    "\n",
    "\n",
    "![link text](https://miro.medium.com/max/1200/0*71shq5eo9xc8LR9R.png)\n",
    "\n",
    "A linear activation function takes the form:\n",
    "\n",
    "y=mx+c ( m is line equation represents W and c is represented as b in neural nets so equation can be modified as y=Wx+b)\n",
    "\n",
    "\n",
    "It takes the inputs (Xi’s), multiplied by the weights(Wi’s) for each neuron, and creates an output proportional to the input. In simple term, weighted sum input is proportional to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLLekYcT1wgb"
   },
   "source": [
    "### Problem with Linear function\n",
    "\n",
    "1. We performed the learning process for neurons with the backpropagation algorithm. This algorithm consists of a derivative system. When y = m.x is derived from x, we reach m. This means that there is no relationship with x. It means that the derivative is always a constant value.\n",
    "\n",
    "So how can we say that the learning process is taking place. The answer is No!\n",
    "\n",
    "2. linear activation functions, no matter how many layers in the neural network, the last layer will be a linear function of the first layer — Meaning Output of the first layer is same as the output of the nth layer. 😐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7aweSQu2L74"
   },
   "source": [
    "A neural networks with a linear activation function is simply a linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeWYHWKp6_ju"
   },
   "source": [
    "## Binary Step Function:\n",
    "\n",
    "![link text](https://miro.medium.com/max/3000/1*QXjftnJ9y5pFwajiB3CG-g.png)\n",
    "\n",
    "It is a function that takes a binary value and is used as a binary classifier.\n",
    "Binary step function are popular known as \" Threshold function\". It is very simple function.\n",
    "\n",
    "Therefore, it is generally preferred in the output layers. It is not recommended to use it in hidden layers because it does not represent derivative learning value and it will not appear in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKG7oVwet_jl"
   },
   "source": [
    "# Non-Linear function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38JS1FBF9vMG"
   },
   "source": [
    "## Sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "C1LaohhA9saC",
    "outputId": "e2dd5af4-493f-4ce5-8030-ff27a8eb6d32"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WsFasV46KgQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just run these cell \n",
    "# this cell is just for yputube video display in notebook\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WsFasV46KgQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZKZiI_w7ig3"
   },
   "source": [
    "\n",
    "\n",
    "![link text](https://miro.medium.com/max/1000/1*Myto4ZQagAOoyom4tqkaRQ.png)\n",
    "\n",
    "\n",
    "Most modern neural network use the non-linear function as their activation function to fire the neuron.\n",
    "It is also derivated because it is different from the step function tha means that learning of neural network can happen.\n",
    "\n",
    "\n",
    "If we examine the graph x is between -2 and +2, y values change quickly. Small changes in x will be large in y. This means that it can be used as a good classifier. \n",
    "\n",
    "Another advantage of this function is that it produces a value in the range of (0,1) when encountered with (- infinite, + infinite) as in the linear function. So the activation value does not vanish, this is good news! 😀\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ9f0ZgiAK1w"
   },
   "source": [
    "### Formula\n",
    "\n",
    "![link text](https://miro.medium.com/max/674/0*pvMD0iSS8Mb2zy6W.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SDS02X09_jcX"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# define a function named sigmoid with parameter x\n",
    "\n",
    "  # return the calculated value by the formula 1/(1+e^(-x))\n",
    "    \n",
    "def sigmoid_func(x):\n",
    "    \n",
    "    return (1/(1+math.exp(-x)))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nL-p0y8xAZjW",
    "outputId": "6d50e645-d5a9-498d-8070-fc3c6e0a90ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999065\n"
     ]
    }
   ],
   "source": [
    "# create a variable named sigmoid_value and store value returned by above sigmoid function after passing value 30\n",
    "\n",
    "sigmoid_value = 30 \n",
    "\n",
    "# print the sigmoid_value\n",
    "\n",
    "print(sigmoid_func(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISdDBzux9e74"
   },
   "source": [
    "## Oop's there is a problem with sigmoid 😑\n",
    "\n",
    "If we look carefully at the graph towards the ends of the function, y values react very little to the changes in x.\n",
    "\n",
    "The derivative values in these regions are very small and converge to 0. This is called the \n",
    "\"vanishing gradient\" and the learning is minimal.\n",
    "\n",
    "\n",
    "if 0, not any learning! When slow learning occurs, the optimization algorithm that minimizes error can be attached to local minimum values and cannot get maximum performance from the artificial neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRWWkFA89zxn"
   },
   "source": [
    "## Hyperbolic Tangent Function (Tanh):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "fq4SnoZX91e3",
    "outputId": "18da93fb-7dd2-409c-cd4e-e9ee98872302"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IbeJl3EIJYg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just run these cell \n",
    "# this cell is just for yputube video display in notebook\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IbeJl3EIJYg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6gikxcn_WlT"
   },
   "source": [
    "\n",
    "\n",
    "![link text](https://miro.medium.com/max/3000/1*51Q7QouspCkOvENni2RwfQ.png)\n",
    "\n",
    "It has a structure very similar to Sigmoid function. However, this time the function is defined as (-1, + 1). The advantage over the sigmoid function is that its derivative is more steep, which means it can get more value. This means that it will be more efficient because it has a wider range for faster learning and grading. But again, the problem of gradients at the ends of the function continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "56Ftchuzu2si"
   },
   "outputs": [],
   "source": [
    "# define a function named tanh with parameter x\n",
    "def tanh(x):\n",
    "    numerator = math.exp(x) - math.exp(-x)\n",
    "    denominator = math.exp(x) + math.exp(-x)\n",
    "    \n",
    "    res = numerator/denominator \n",
    "    return res\n",
    "\n",
    "  # return the calculated value by the formula tanh(x)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqbQNwB7Bh4V",
    "outputId": "3636039a-1192-419b-d168-967da9b56e93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# create a variable named tanh_value and store value returned by above tanh function after passing value 30\n",
    "\n",
    "x = 30 \n",
    "\n",
    "# print the tanh_value\n",
    "print(tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6muqFnA_-yft"
   },
   "source": [
    "## ReLu Activation Function (ReLu — Rectified Linear Units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "7xMaRAQi-zqk",
    "outputId": "b590ee51-a743-4c69-9bc7-3aa22efc7c98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/68BZ5f7P94E\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just run these cell \n",
    "# this cell is just for yputube video display in notebook\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/68BZ5f7P94E\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pXsa2haBwzb"
   },
   "source": [
    "\n",
    "\n",
    "![link text](https://miro.medium.com/max/1000/1*m_0v2nY5upLmCU-0SuGZXg.png)\n",
    "\n",
    "![link text](https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)\n",
    "\n",
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.\n",
    "\n",
    "Range: [ 0 to infinity]\n",
    "\n",
    "Main advantage of using the ReLU function- It does not activate all the neurons at the same time.\n",
    "It converges very fast\n",
    "It is computationally efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WKt3o_jzu6ys"
   },
   "outputs": [],
   "source": [
    "# define a function named relu with parameter x\n",
    "def relu(x):\n",
    "    return max(0,x)\n",
    "  # return the maximum values in x greater than zero i.e All positive numbers\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFQ5e2Q2FLTP",
    "outputId": "adf627f1-fc91-4df0-d9a6-8cd7cf6beaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# create a variable named relu_value and store value returned by above relu function after passing value -50\n",
    "x = -50 \n",
    "print(relu(x))\n",
    "# print relu_value\n",
    "\n",
    "print(relu(120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2RqhK0sCJzo"
   },
   "source": [
    "## leaky ReLu Activation Function:\n",
    "\n",
    "![link text](https://miro.medium.com/max/625/1*gDIUV3yonKbIWh_9Kl4ShQ.png)\n",
    "\n",
    "Leaky ReLU function is nothing but an improved version of the ReLU function with introduction of \"constant slope\".\n",
    "\n",
    "It has a small slope for negative values instead of a flat slope\n",
    "\n",
    "Leaky ReLU is defined to address problem of dying neuron/dead neuron.\n",
    "\n",
    "It allows negative value during back propagation\n",
    "\n",
    "\n",
    "### Note:\n",
    "\n",
    "Leaky ReLU does not provide consistent predictions for negative input values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8_Zf90aJu6vU"
   },
   "outputs": [],
   "source": [
    "# define a function named relu with parameter x\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return max(x,0.01*x)\n",
    "\n",
    "  # return the maximum values between x and 0.01*x \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNQSY7FCHqwb",
    "outputId": "ad5454a2-1b58-45b8-fe12-aa281795bc34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# create a variable named leaky_relu_value and store value returned by above leakyRelu function after passing value -10\n",
    "\n",
    "x = -10 \n",
    "print(leaky_relu(x))\n",
    "# print leaky_relu_value\n",
    "print(leaky_relu(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXSTXkqBCOZ6"
   },
   "source": [
    "## ELU (Exponential Linear Units) Activation Function:\n",
    "\n",
    "![link text](https://360digit.b-cdn.net/assets/admin/ckfinder/userfiles/images/blog/elu.png)\n",
    "\n",
    "ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "\n",
    "It is a function that tend to converge cost to zero faster and produce more accurate results\n",
    "\n",
    "###  Note:\n",
    "\n",
    "With values higher values above 0 the Elu function blows up the activation beacuse the number can lie between [0,infinity]\n",
    "\n",
    "Similar to Leaky ReLU, although theoretically better than ReLU, there is currently no good evidence in practice that ELU is always better than ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zDMd7CUwCNhD"
   },
   "outputs": [],
   "source": [
    "# define a function named elu with parameter x\n",
    "\n",
    "def elu(x):\n",
    "    return max(x,0.01*(math.exp(x)-1))\n",
    "  # return the maximum values between x and 0.01*((e^x) - 1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akDaKbVQKxL9",
    "outputId": "c7abfd4b-447f-4bb4-f9d4-1fe7cf6da850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.009999546000702375\n",
      "-0.01\n"
     ]
    }
   ],
   "source": [
    "# create a variable named elu_value and store value returned by above elu function after passing value -10\n",
    "x = -10 \n",
    "print(elu(x))\n",
    "# print elu_value\n",
    "print(elu(-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAE6u4ETCS9C"
   },
   "source": [
    "## P ReLu (Parametric ReLU) Activation Function:\n",
    "\n",
    "![link text](https://miro.medium.com/max/875/0*8VBU_GxQ07OvZN2q.png)\n",
    "\n",
    "\n",
    "A Parametric Rectified Linear Unit, or PReLU, is an activation function that generalizes the traditional rectified unit with a slope for negative values.\n",
    "\n",
    "Whenever the x value will be greater than zero itwill give a output value as x, but whenever the x value is below zero it will give the value as aplha*x\n",
    "\n",
    "### Pay Attention\n",
    "\n",
    "1. If alpha value is 0.01 then Prelu function will become Leaky Relu function\n",
    "\n",
    "2. If alpha value is 0 then Prelu will become Relu function\n",
    "\n",
    "3. also aplha can be any value we want.\n",
    "\n",
    "Thus the alpha here is named as \"learning parameter\" which will dynamicaly change based on the this particularactivation function.\n",
    "\n",
    "Therefore depending on the parameter the function changes to Relu, Leaky Relu, Elu, etc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T3ZwhQlxu6tK"
   },
   "outputs": [],
   "source": [
    "# define a function named PRelu with parameter x, alpha\n",
    "\n",
    "def PRelu(x,alpha):\n",
    "    return max(x,alpha*x)\n",
    "\n",
    "  # return the maximum values between x and alpha*x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZCS0eQ2PV4k",
    "outputId": "9f6e2c57-64a7-4e09-83b2-7aed45c81226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# create a variable named p_relu_value and store value returned by above PRelu function after passing value x = -10 and alpha = 0.01 \n",
    "\n",
    "x,alpha = -10,0.01\n",
    "# print p_relu_value\n",
    "print(PRelu(x,alpha))\n",
    "print(PRelu(-100,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pExsbFeCA9bn"
   },
   "source": [
    "## Swish (A Self-Gated) Activation Function:(Sigmoid Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "zXw9cR_oA5wQ",
    "outputId": "88d23ecb-70f4-4cac-9ae7-ed1c531fc2c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2BCxzWji1rQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2BCxzWji1rQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEZq3In0CXWN"
   },
   "source": [
    "\n",
    "![link text](https://miro.medium.com/max/625/1*kFWX1dWUtLWo3LJLKsqhwA.png)\n",
    "\n",
    "\n",
    "formula:\n",
    "\n",
    "y = x * sigmoid(x)\n",
    "\n",
    "\n",
    "Swish is a smooth continuous function, unlike ReLU which is a piecewise linear function. Swish allows a small number of negative weights to be propagated through, while ReLU thresholds all negative weights to zero. This is an extremely important property and is crucial in the success of non-monotonic smooth activation functions, like that of Swish, when used in increasingly deep neural networks. Lastly, the trainable parameter allows to better tune the activation function to maximize information propagation and push for smoother gradients, which makes the landscape easier to optimize, thus generalizing better and faster. Swish is also a self-gating activation function since it modulates the input by using it as a gate to multiply with the sigmoid of itself, a concept first introduced in Long Short-Term Memory (LSTMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JMNX6HzBu6rF"
   },
   "outputs": [],
   "source": [
    "# define a function named swish with parameter x\n",
    "\n",
    "def swish(x):\n",
    "    return (x*(1/(1+math.exp(-x))))\n",
    "  # return the calculated value by the formula x * 1/(1+e^(-x))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrqmOQalR3bS",
    "outputId": "f5cdfd24-0035-4ecb-c3df-dc1003c28c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.999999999997197\n"
     ]
    }
   ],
   "source": [
    "# create a variable named swish_value and store value returned by above swish function after passing value x = 30  \n",
    "\n",
    "x = 30 \n",
    "print(swish(x))\n",
    "# print swish_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAA49OzzCews"
   },
   "source": [
    "## Softplus\n",
    "\n",
    "![link text](https://miro.medium.com/max/875/0*492PBXPSjBYH3nsI.png)\n",
    "\n",
    "Formula:\n",
    "\n",
    "y = ln(1+e^x)\n",
    "\n",
    "Softplus activation function is a smooth version of ReLU. The name ‘softplus’ is used because of the smoothed or softened version of ReLU.\n",
    "\n",
    "There is no need to find derivative of 0 as compared to Relu\n",
    "\n",
    "It enhances the stability and performance of a Deep Neural Network architecture because of smoothness and non-zero gradient.\n",
    "\n",
    "The unboundedness in upper limit and boundedness in the lower limit helps the network to avoid saturation and induce regularization respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S_fMJzneu6oi"
   },
   "outputs": [],
   "source": [
    "# define a function named softPlus with parameter x\n",
    "def softPlus(x):\n",
    "    \n",
    "    return np.log(1+(math.exp(x)))\n",
    "\n",
    "  # return the calculated value by the formula ln(1+e^(x))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiCyLDtGUt9J",
    "outputId": "0352fc62-451c-40e2-a2a9-0435a43d1be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.000000000000092\n"
     ]
    }
   ],
   "source": [
    "# create a variable named softplus_value and store value returned by above swish function after passing value x = 30  \n",
    "x = 30\n",
    "print(softPlus(x))\n",
    "# print softplus_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDwOpijrAVXa"
   },
   "source": [
    "## Softmax or normalized exponential function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "uBasTHcvAHvw",
    "outputId": "4a70cf9f-030b-4a9e-cf3b-52954fbf9d90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/o6HrH2EMD-w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just run these cell \n",
    "# this cell is just for yputube video display in notebook\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/o6HrH2EMD-w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxzkmzT-ChPN"
   },
   "source": [
    "Formula:\n",
    "\n",
    "![link text](https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg)\n",
    "\n",
    "\n",
    "Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector.\n",
    "\n",
    "The most common use of the softmax function in applied machine learning is in its use as an activation function in a neural network model. Specifically, the network is configured to output N values, one for each class in the classification task, and the softmax function is used to normalize the outputs, converting them from weighted sum values into probabilities that sum to one. Each value in the output of the softmax function is interpreted as the probability of membership for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "JtWPALFBWtpI"
   },
   "outputs": [],
   "source": [
    "# define a function named softmax with parameter x\n",
    "\n",
    "from numpy import exp\n",
    "def softmax(x):\n",
    "    output = []\n",
    "    \n",
    "    expos = exp(x)\n",
    "    \n",
    "    output.append(expos/expos.sum())\n",
    "    \n",
    "    \n",
    "    return output\n",
    "    \n",
    "\n",
    "  # create empty list named output\n",
    "\n",
    "  \n",
    "  # create a loop in range of length of input list\n",
    "  \n",
    "    # create a empty list named exp_values to store exponential values of each element in input list\n",
    "    \n",
    "    # create a loop in range of length of input list\n",
    "    \n",
    "      # create a variable named exp_value to store current iterated elemnts exponential value\n",
    "      \n",
    "      # append exp_value to the empty list created above named exp_values\n",
    "      \n",
    "    \n",
    "\n",
    "    # Append the result of calculation e^xi/sum(e^xi) to output list created at the top\n",
    "    \n",
    "  \n",
    "  # Convert the list to array using np.array\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoJQ1UT5u6mP",
    "outputId": "469dd0ac-46c3-455a-9266-781a72d3f779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.0015683 , 0.00426308, 0.01158826, 0.03150015, 0.0856263 ,\n",
      "       0.2327564 , 0.6326975 ])]\n"
     ]
    }
   ],
   "source": [
    "# create a list named input with random values and any length you want.\n",
    "\n",
    "inp = [ 1,2,3,4,5,6,7]\n",
    "softmax_value = softmax(inp)\n",
    "print(softmax_value)\n",
    "# create a variable named softmax_value and store value returned by above softmax function after passing value x = [x1,x2,x3,x4,x5....]  i.e input array/list\n",
    "\n",
    "# print softmax_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWlQL3cIZrCv"
   },
   "source": [
    "The above output shows the probailities of the the input array passed to it, with highest probaility of number 5.1 as 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItdKBbVTrhIU"
   },
   "source": [
    "# Assignment Summary\n",
    "\n",
    "When creating a neural network we must select a apropriate activation function accordingly: \n",
    "\n",
    "### following are the activation function discussed in this assignment\n",
    "\n",
    "1. Sigmoid\n",
    "2. Tanh\n",
    "3. Relu\n",
    "4. Leaky Relu\n",
    "5. Elu\n",
    "6. PRelu\n",
    "7. Swish\n",
    "8. Softplus\n",
    "9. Softmax\n",
    "\n",
    "Relu is used in most of the neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbqhEi7oqkKT"
   },
   "source": [
    "![link text](https://image.shutterstock.com/image-vector/congratulations-greeting-sign-graduation-party-260nw-1396729610.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXvP_0qUqPj7"
   },
   "source": [
    "# we have learned all the activation function used in neural network and their properties with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9rXbwNj-3Or"
   },
   "source": [
    "# **Do Fill The Feedback Form**\n",
    "\n",
    "\n",
    "https://forms.zohopublic.in/cloudyml/form/CloudyMLDeepLearningFeedbackForm/formperma/VCFbldnXAnbcgAIl0lWv2blgHdSldheO4RfktMdgK7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P03C8FtP_Bfw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Activation_functions-without-code.ipynb",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}